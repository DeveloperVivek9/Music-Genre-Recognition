{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bb42a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39796d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Audio-data augmentation\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "#Various audio processing functionalities\n",
    "import librosa\n",
    "#Just to find the ceil \n",
    "import math\n",
    "#Visualization of model history\n",
    "import matplotlib.pyplot as plt\n",
    "#Memory maps and various computations involving arrays\n",
    "import numpy as np\n",
    "#File read/information retrieval\n",
    "import os\n",
    "#CSV file manipulation\n",
    "import pandas as pd\n",
    "#To shuffle and split the dataset. We use it here to split indices of memory maps.\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import time\n",
    "from tensorflow.python.keras.saving import hdf5_format\n",
    "import h5py\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9425ef",
   "metadata": {},
   "source": [
    "##### Assumptions \n",
    "- Any class in csv file should have a viable number of songs in them. The amount of songs we take for training and testing is equivalent to the n(songs) of that class which has the minimum amount of songs -> to balance the dataset.\n",
    "- Don't have variables named X,y,X_train ,X_validation,X_test,y_train,y_validation,y_test in this script since it will either get deleted or the memory maps won't get created. We are using global so warning\n",
    "- Memory maps on creation have a default value of 0 array with shape we mention on creation  [To extract the actual size of added data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8679fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Augment_Song(signal,sample_rate,n_augmentations):\n",
    "    signals=list()\n",
    "    signals.append(signal)\n",
    "    #print(\"Original\")\n",
    "    if(n_augmentations<=0):\n",
    "        return signals \n",
    "    transform1 = AddGaussianNoise(min_amplitude=0.001,max_amplitude=0.015,p=1.0)\n",
    "    augmented_sound = transform1(signal, sample_rate=sample_rate)\n",
    "    signals.append(augmented_sound)\n",
    "    #print(\"Gaussian Noise\")\n",
    "    if(n_augmentations<=1):\n",
    "        return signals \n",
    "    transform2 =  TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5)\n",
    "    augmented_sound = transform2(signal, sample_rate=sample_rate)\n",
    "    signals.append(augmented_sound)\n",
    "    #print(\"Time Stretch\")\n",
    "    if(n_augmentations<=2):\n",
    "        return signals \n",
    "    transform3=PitchShift(min_semitones=-4, max_semitones=4, p=0.5)\n",
    "    augmented_sound=transform3(signal, sample_rate=sample_rate)\n",
    "    signals.append(augmented_sound)\n",
    "    #print(\"Pitch Shift\")\n",
    "    if(n_augmentations<=3):\n",
    "        return signals \n",
    "    transform4=Shift(min_fraction=-0.5, max_fraction=0.5, p=0.5)\n",
    "    augmented_sound=transform4(signal, sample_rate=sample_rate)\n",
    "    signals.append(augmented_sound)\n",
    "    #print(\"Shift\")\n",
    "    if(n_augmentations<=4):\n",
    "        return signals \n",
    "    transform5= Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    PitchShift(min_semitones=-1, max_semitones=1, p=0.5),\n",
    "    Shift(min_fraction=-0.2, max_fraction=0.2, p=0.5),\n",
    "    ])\n",
    "    augmented_sound=transform5(signal, sample_rate=sample_rate)\n",
    "    signals.append(augmented_sound)\n",
    "    #print(\"Combined Transforms\") \n",
    "    return signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33ad5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(DATASET_PATH,CSV_PATH):\n",
    "    \"\"\"\n",
    "    DATASET_PATH -> Directory which will have folders (classes) of audio files.\n",
    "    CSV_PATH -> Path of csv file\n",
    "    CSV file structure : ['Filename','Genre','Path']\n",
    "        Filename is the name of the audio file including extension\n",
    "        Genre will be the name of the parent folder of the audio file. This assumes audio files are stored in folders having their respective class (genre) name.\n",
    "        Path will be the path of the audio file including the filename as well.\n",
    "    \"\"\"\n",
    "    df=pd.DataFrame(list(),columns=[\"Filename\",\"Genre\",\"Path\"])\n",
    "    for genre_folder in next(os.walk(DATASET_PATH))[1]:\n",
    "        for file in next(os.walk(os.path.join(DATASET_PATH,genre_folder)))[2]:\n",
    "            df.loc[len(df.index)]=[file,genre_folder.title(),os.path.join(DATASET_PATH,genre_folder,file)]\n",
    "    df.to_csv(CSV_PATH,index=False)\n",
    "    print(f\"Path of the created csv file : {CSV_PATH}. If the dataset doesn't change in the future, Pass the path of that csv file as the function parameter to avoid writing csv file over and over again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57a3cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_csv(CSV_PATH,TEST_CSV_PATH,test_percentage=10):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Decide test_set percentage for a viable observation of performance\n",
    "    #create test_csv file having 1% of main_csv file\n",
    "    #remove test from main_csv and add (new calculated 1%-existing test size) to it as the main_csv file grows and evaluate previous models on new test_csv file\n",
    "    df=pd.read_csv(CSV_PATH)\n",
    "    df=df.drop_duplicates(['Path'])\n",
    "    genre_count_list=list(df[\"Genre\"].value_counts())\n",
    "    required_test_set_size=int(min(genre_count_list)/test_percentage)\n",
    "    print(f\"Aiming for test size : {required_test_set_size}\")\n",
    "    try:\n",
    "        test_df=pd.read_csv(TEST_CSV_PATH)\n",
    "        test_df=test_df.drop_duplicates(['Path'])\n",
    "        for genre in list(df[\"Genre\"].unique()):\n",
    "            try:\n",
    "                current_test_set_size_for_genre=test_df['Genre'].value_counts()[genre]\n",
    "            #If genre doesn't exist in test csv file\n",
    "            except KeyError:\n",
    "                print(f\"Genre : {genre} doesn't exist in test csv file.\")\n",
    "                current_test_set_size_for_genre=0\n",
    "            if(current_test_set_size_for_genre>required_test_set_size):\n",
    "                print(f\"Test csv contains {current_test_set_size_for_genre} records for {genre} which is more than required size : {required_test_set_size}. Removing {current_test_set_size_for_genre-required_test_set_size} records from test.\")\n",
    "                #Taking a random sample equal to the extra elements in test.\n",
    "                temp_df=test_df.loc[test_df[\"Genre\"]==genre].sample(n=current_test_set_size_for_genre-required_test_set_size)\n",
    "                #Removing that sample from test csv\n",
    "                test_df=pd.merge(test_df,temp_df, indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1)\n",
    "            elif(current_test_set_size_for_genre<required_test_set_size):\n",
    "                print(f\"Required test size is : {required_test_set_size} for {genre} which is more than current test size : {current_test_set_size_for_genre}. Adding {required_test_set_size-current_test_set_size_for_genre} records to test.\")\n",
    "                #Removing rows that already exist in test csv file\n",
    "                temp_df=pd.merge(df,test_df, indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1)\n",
    "                #Then taking a random sample so that our test size reaches required size.\n",
    "                temp_df=temp_df.loc[temp_df[\"Genre\"]==genre].sample(n=required_test_set_size-current_test_set_size_for_genre)\n",
    "                #Adding that sample to test csv\n",
    "                test_df=pd.concat([test_df, temp_df], ignore_index=True, sort=False)\n",
    "            else:\n",
    "                pass\n",
    "        test_df.to_csv(TEST_CSV_PATH,index=False)\n",
    "    except FileNotFoundError:\n",
    "        test_df=pd.DataFrame(list(),columns=['Filename','Artist','Title','Genre','Path','Source'])\n",
    "        for genre in list(df[\"Genre\"].unique()):\n",
    "            #Taking a random sample from the dataset\n",
    "            temp_df=df.loc[df[\"Genre\"]==genre].sample(n=required_test_set_size)\n",
    "            test_df = pd.concat([test_df, temp_df], ignore_index=True, sort=False)\n",
    "        test_df.to_csv(TEST_CSV_PATH,index=False)\n",
    "        print(f\"Path of the created test csv file : {TEST_CSV_PATH}. Pass the path for future iterations.\")   \n",
    "    display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a92d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df,MEMORY_MAPPING_PATH,ERROR_PATH,SAMPLE_RATE,TRACK_DURATION,n_mfcc,n_fft,hop_length,n_segments,n_augmentations):\n",
    "    try:\n",
    "        #Deleting memory map files so that we don't face issues when creating memory maps here.\n",
    "        clean(MEMORY_MAPPING_PATH,\"files\",file_list=[\"X\",\"y\"])\n",
    "        try:\n",
    "            error_df=pd.read_csv(ERROR_PATH)\n",
    "        except FileNotFoundError:\n",
    "            error_df=pd.DataFrame(list(),columns=['Filename','Genre','Path','Segment','Error'])\n",
    "\n",
    "        genre_list=sorted(list(df[\"Genre\"].unique()))\n",
    "        print(f\"Classes present in the dataframe : {genre_list}\")\n",
    "        SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
    "        samples_per_segment = int(SAMPLES_PER_TRACK / n_segments)\n",
    "        n_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "        \n",
    "        max_input_size=len(df.index)*(n_augmentations+1)*n_segments\n",
    "        temp_X = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"X.mymemmap\"), dtype='float64', mode='w+', shape=(max_input_size, n_mfcc_vectors_per_segment, n_mfcc))\n",
    "        temp_y = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"y.mymemmap\"), dtype='uint8', mode='w+', shape=(max_input_size,))\n",
    "        print(\"Empty memortemp_y maps X.mymemmap and y.mymemmap created.\")\n",
    "\n",
    "        #audio file count\n",
    "        song_count=1\n",
    "        #viable array count\n",
    "        count=0\n",
    "        test_null_signal_array=np.zeros((n_mfcc_vectors_per_segment,n_mfcc))\n",
    "        test_null_signal_array[:,0]=-1131.370849898476\n",
    "        for index,row in df.iterrows():\n",
    "            starting_time=time.time()\n",
    "            print(f\"{song_count}. {row['Filename']} : {genre_list.index(row['Genre'])}\")\n",
    "            try:\n",
    "                signal, sample_rate = librosa.load(row['Path'], sr=SAMPLE_RATE,duration=TRACK_DURATION)\n",
    "            except Exception as e:\n",
    "                error_df.loc[len(error_df.index)]=[row[\"Filename\"],row[\"Genre\"],row[\"Path\"],\"\",e]\n",
    "                print(\"ERROR : Couldn't read file\")\n",
    "                continue\n",
    "            signals=Augment_Song(signal,sample_rate,n_augmentations)\n",
    "            for signal in signals:\n",
    "                for d in range(n_segments):\n",
    "                    start = samples_per_segment * d\n",
    "                    finish = start + samples_per_segment\n",
    "                    #n_mfcc values beyond 128 produces 128 mfcc only.\n",
    "                    mfcc = librosa.feature.mfcc(y=signal[start:finish], sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "                    mfcc = mfcc.T\n",
    "                    \n",
    "                    #This check makes sure the shape of the input is consistent. Assuming this stays in, we can assume shape to be (n_mfcc_vectors_per_segment,n_mfcc) for each segment\n",
    "                    if(mfcc.shape==(n_mfcc_vectors_per_segment,n_mfcc)):\n",
    "                        if(np.all(np.equal(test_null_signal_array,mfcc))):\n",
    "                            print(f\"ERROR : Segment {d} skipped.\")\n",
    "                            error_df.loc[len(error_df.index)]=[row[\"Filename\"],row[\"Genre\"],row[\"Path\"],d,\"mfcc array equal to array produced if signal is a zero array.\"]\n",
    "                            continue\n",
    "                        if(np.any(np.isnan(mfcc))):\n",
    "                            print(f\"ERROR : Segment {d} skipped.\")\n",
    "                            error_df.loc[len(error_df.index)]=[row[\"Filename\"],row[\"Genre\"],row[\"Path\"],d,\"Null value present in mfcc array.\"]\n",
    "                            continue\n",
    "                        if(np.any(np.isinf(mfcc))):\n",
    "                            print(f\"ERROR : Segment {d} skipped.\")\n",
    "                            error_df.loc[len(error_df.index)]=[row[\"Filename\"],row[\"Genre\"],row[\"Path\"],d,\"Inf value present in mfcc array.\"]\n",
    "                            continue\n",
    "                        temp_X[count,:]=mfcc\n",
    "                        temp_y[count]=genre_list.index(row['Genre'])\n",
    "                        count+=1\n",
    "                    else:\n",
    "                        print(f\"ERROR : Segment {d} skipped.\")\n",
    "                        error_df.loc[len(error_df.index)]=[row[\"Filename\"],row[\"Genre\"],row[\"Path\"],d,\"Shape of mfcc array not consistent with our calculation\"]\n",
    "            song_count+=1\n",
    "        error_df.to_csv(ERROR_PATH,index=False)\n",
    "        ending_time=time.time()\n",
    "        del temp_X\n",
    "        del temp_y\n",
    "        gc.collect()\n",
    "        print(\"Data loaded : \",ending_time-starting_time,\"seconds\",end=\"\\n\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception occured :\",e)\n",
    "        try:\n",
    "            del temp_X\n",
    "        except NameError:\n",
    "            pass\n",
    "        try:\n",
    "            del temp_y\n",
    "        except NameError:\n",
    "            pass\n",
    "        gc.collect()\n",
    "        clean(MEMORY_MAPPING_PATH,\"files\",file_list=[\"X\",\"y\"])\n",
    "        raise Exception(e)\n",
    "    except KeyboardInterrupt:\n",
    "        #If user interrupts this process, X and y can have corrupt values since it was in the middle of adding data. \n",
    "        #We are removing the memory map files as well as deleting the variables for safety.\n",
    "        #deleting variables required since referenced to the memory map will still be in memory \n",
    "        #which will not allow us to run the code again since WinError which tells us some other process is using the file.\n",
    "        try:\n",
    "            del temp_X\n",
    "        except NameError:\n",
    "            pass\n",
    "        try:\n",
    "            del temp_y\n",
    "        except NameError:\n",
    "            pass\n",
    "        gc.collect()\n",
    "        clean(MEMORY_MAPPING_PATH,\"files\",file_list=[\"X\",\"y\"])\n",
    "        raise KeyboardInterrupt(\"Deleted memory maps and variables from memory since values will be corrupt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef79a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(MEMORY_MAPPING_PATH,s,file_list=None):\n",
    "    #if(s==\"csv\")\n",
    "    if(file_list==None):\n",
    "        file_list=[\"X\",\"y\",\"X_train\",\"X_validation\",\"X_test\",\"y_train\",\"y_validation\",\"y_test\"]\n",
    "    if((s==\"variables\") or (s==\"full\")):\n",
    "        for variable in file_list:\n",
    "            #myVars = locals() \n",
    "            #del doesn't work if locals since it returns a copy of the directory not the directory itself like globals. So for our usecase, we have to use globals [may be dangerous.]\n",
    "            myVars = globals()\n",
    "            try:\n",
    "                del myVars[variable]\n",
    "            except NameError:\n",
    "                continue\n",
    "            except KeyError:\n",
    "                continue\n",
    "        gc.collect()\n",
    "    if((s==\"files\") or (s==\"full\")):\n",
    "        for file in file_list:\n",
    "            try:\n",
    "                os.remove(os.path.join(MEMORY_MAPPING_PATH,file+\".mymemmap\"))\n",
    "            except FileNotFoundError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ba944f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(MEMORY_MAPPING_PATH,ERROR_PATH,CSV_PATH,TEST_CSV_PATH,SAMPLE_RATE,TRACK_DURATION,n_mfcc,n_fft,hop_length,n_segments,n_augmentations):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Function_start_time=time.time()\n",
    "    \n",
    "    global X\n",
    "    global y\n",
    "    global X_train \n",
    "    global X_validation\n",
    "    global X_test\n",
    "    global y_train\n",
    "    global y_validation\n",
    "    global y_test \n",
    "    \n",
    "    \n",
    "    df=pd.read_csv(CSV_PATH)\n",
    "    print(\"CSV file read.\")\n",
    "    df=df.drop_duplicates(['Path'])\n",
    "    \n",
    "    test_percentage=10\n",
    "    create_test_csv(CSV_PATH,TEST_CSV_PATH,test_percentage)\n",
    "    test_df=pd.read_csv(TEST_CSV_PATH)\n",
    "    df=pd.merge(df,test_df, indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1)\n",
    "    genre_count_list=list(df[\"Genre\"].value_counts())\n",
    "    balanced_df=pd.DataFrame(list(),columns=['Filename','Artist','Title','Genre','Path','Source'])\n",
    "    for genre in list(df[\"Genre\"].unique()):\n",
    "        #Taking a random sample from the dataset\n",
    "        temp_df=df.loc[df[\"Genre\"]==genre].sample(n=min(genre_count_list))\n",
    "        balanced_df = pd.concat([balanced_df, temp_df], ignore_index=True, sort=False)\n",
    "    print(f\"Dataset balanced to have only {min(genre_count_list)} data points per class.\")\n",
    "    #To shuffle. Not needed since we are using train test split for shuffling which is a better way since this will only shuffle songs not segments as well.\n",
    "    #df=df.sample(n = len(df.index))\n",
    "    \n",
    "    #delete memory mapping if present in directory\n",
    "    file_list=[\"X\",\"y\",\"X_train\",\"X_validation\",\"X_test\",\"y_train\",\"y_validation\",\"y_test\"]\n",
    "    clean(MEMORY_MAPPING_PATH,\"full\",file_list)\n",
    "    try:\n",
    "        os.remove(ERROR_PATH)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
    "        samples_per_segment = int(SAMPLES_PER_TRACK / n_segments)\n",
    "        n_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "        #Loading for training set.\n",
    "        load_data(df,MEMORY_MAPPING_PATH,ERROR_PATH,SAMPLE_RATE,TRACK_DURATION,n_mfcc,n_fft,hop_length,n_segments,n_augmentations)\n",
    "        max_input_size=len(balanced_df.index)*(n_augmentations+1)*n_segments\n",
    "        X = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"X.mymemmap\"), dtype='float64', mode='r', shape=(max_input_size, n_mfcc_vectors_per_segment, n_mfcc))\n",
    "        y = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"y.mymemmap\"), dtype='uint8', mode='r', shape=(max_input_size,))\n",
    "        #Getting the actual size of the training set since we might encounter segment errors and unreadable files.\n",
    "        actual_size=len(X)\n",
    "        d=np.zeros((n_mfcc_vectors_per_segment, n_mfcc),dtype=float)\n",
    "        for i in range(-1,-len(X),-1):\n",
    "            if(np.array_equal(X[-i],d)):\n",
    "                actual_size-=1\n",
    "        index_list=np.arange(0, actual_size, 1, dtype=int)\n",
    "        #Splitting from training set, the validation set.\n",
    "        validation_ratio=0.2\n",
    "        train_indices,validation_indices=train_test_split(index_list,test_size=validation_ratio,shuffle=True)\n",
    "        print(f\"Splitting X in the ratio : {validation_ratio}. \\n(Training + Validation) set size : {actual_size}. \\nTrain set size : {len(train_indices)} \\nValidation set size : {len(validation_indices)}\")\n",
    "\n",
    "        #Creating empty memory maps for train and validation\n",
    "        X_train = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"X_train.mymemmap\"), dtype='float64', mode='w+', shape=(len(train_indices), n_mfcc_vectors_per_segment, n_mfcc))\n",
    "        X_validation = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"X_validation.mymemmap\"), dtype='float64', mode='w+', shape=(len(validation_indices), n_mfcc_vectors_per_segment, n_mfcc))\n",
    "        y_train = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"y_train.mymemmap\"), dtype='uint8', mode='w+', shape=(len(train_indices),))\n",
    "        y_validation = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"y_validation.mymemmap\"), dtype='uint8', mode='w+', shape=(len(validation_indices),))\n",
    "        print(\"Empty memory maps created for train and validation\")\n",
    "\n",
    "        #Placing respective values in validation and train memory maps.\n",
    "        for index in range(0,actual_size):\n",
    "            #print(index,end=\":\")\n",
    "            if(index in train_indices):\n",
    "                #print(\"train\")\n",
    "                X_train[np.where(train_indices == index)[0][0],:]=X[index]\n",
    "                y_train[np.where(train_indices == index)[0][0]]=y[index]\n",
    "            elif(index in validation_indices):\n",
    "                #print(\"validation\")\n",
    "                X_validation[np.where(validation_indices == index)[0][0],:]=X[index]\n",
    "                y_validation[np.where(validation_indices == index)[0][0]]=y[index]\n",
    "            else:\n",
    "                print(f\"ERROR : Index {count} not in train or validation indices\")\n",
    "       \n",
    "        print(\"Train and validation memory maps written.\")\n",
    "        clean(MEMORY_MAPPING_PATH,\"variables\",[\"X\",\"y\"])\n",
    "        \n",
    "        #Avoiding augmentation here since we assume test to be the actual real representation that we want to perform well on.\n",
    "        load_data(test_df,MEMORY_MAPPING_PATH,ERROR_PATH,SAMPLE_RATE,TRACK_DURATION,n_mfcc,n_fft,hop_length,n_segments,0)\n",
    "        max_input_size=len(test_df.index)*(n_augmentations+1)*n_segments\n",
    "        X = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"X.mymemmap\"), dtype='float64', mode='r', shape=(max_input_size, n_mfcc_vectors_per_segment, n_mfcc))\n",
    "        y = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"y.mymemmap\"), dtype='uint8', mode='r', shape=(max_input_size,))\n",
    "        actual_size=len(X)\n",
    "        for i in range(-1,-len(X),-1):\n",
    "            if(np.array_equal(X[-i],d)):\n",
    "                actual_size-=1\n",
    "        print(f\"Test set size : {actual_size}\")\n",
    "        X_test = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"X_test.mymemmap\"), dtype='float64', mode='w+', shape=(actual_size, n_mfcc_vectors_per_segment, n_mfcc))\n",
    "        y_test = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"y_test.mymemmap\"), dtype='uint8', mode='w+', shape=(actual_size,))\n",
    "        X_test[:]=X[:actual_size]\n",
    "        y_test[:]=y[:actual_size]\n",
    "        print(\"Test memory map written.\")\n",
    "        clean(MEMORY_MAPPING_PATH,\"variables\",file_list)\n",
    "    except Exception as e:\n",
    "        print(\"Exception occured : \",e)\n",
    "        clean(MEMORY_MAPPING_PATH,\"full\",file_list)\n",
    "        raise Exception(e)\n",
    "    except KeyboardInterrupt:\n",
    "        clean(MEMORY_MAPPING_PATH,\"full\",file_list)\n",
    "        raise KeyboardInterrupt(\"Deleted memory maps and variables from memory since values will be corrupt\")\n",
    "    Function_end_time=time.time()\n",
    "    print(f\"\\n\\n\\nPreprocessing Execution Time : {Function_end_time-Function_start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c73c7679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape,n_classes):\n",
    "    \"\"\"Generates CNN model\n",
    "\n",
    "    :param input_shape (tuple): Shape of input set\n",
    "    :return model: CNN model\n",
    "    \"\"\"\n",
    "\n",
    "    # build network topology\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # 1st conv layer\n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    # 2nd conv layer\n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    # 3rd conv layer\n",
    "    model.add(keras.layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    # flatten output and feed it into dense layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "    # output layer\n",
    "    model.add(keras.layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910a3114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
    "\n",
    "        :param history: Training history of model\n",
    "        :return:\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    # create accuracy sublpot\n",
    "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
    "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy eval\")\n",
    "\n",
    "    # create error sublpot\n",
    "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
    "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
    "    axs[1].set_ylabel(\"Error\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Error eval\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "992cc8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, y):\n",
    "    \"\"\"Predict a single sample using the trained model\n",
    "\n",
    "    :param model: Trained classifier\n",
    "    :param X: Input data\n",
    "    :param y (int): Target\n",
    "    \"\"\"\n",
    "\n",
    "    # add a dimension to input data for sample - model.predict() expects a 4d array in this case\n",
    "    X = X[np.newaxis, ...] # array shape (1, 130, 13, 1)\n",
    "    #(130,13) array produced for each segment. Therefore (n_segments,130,13)\n",
    "\n",
    "    # perform prediction\n",
    "    prediction = model.predict(X)\n",
    "\n",
    "    # get index with max value\n",
    "    predicted_index = np.argmax(prediction, axis=1)\n",
    "\n",
    "    print(\"Target: {}, Predicted label: {}\".format(y, predicted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d927a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(MEMORY_MAPPING_PATH,n_mfcc,n_segments,n_mfcc_vectors_per_segment):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    s=time.time()\n",
    "    memory_map_directory_file_list=next(os.walk(MEMORY_MAPPING_PATH))[2]\n",
    "    required_memory_maps_list=[\"X_train.mymemmap\",\"X_validation.mymemmap\",\"X_test.mymemmap\",\"y_train.mymemmap\",\"y_validation.mymemmap\",\"y_test.mymemmap\"]\n",
    "    #If the required maps not in the given directory\n",
    "    if(not(all(item in memory_map_directory_file_list for item in required_memory_maps_list))):\n",
    "        raise ValueError('Memory Map directory may be incorrect since required memory maps not found. Use remove=True if memory maps were not created during the previous iterations.')\n",
    "        \n",
    "    #Here 8 is because float64 uses 8 bytes. [float64 uses 64 bits -> 8 bytes]\n",
    "    file_size = os.path.getsize(os.path.join(MEMORY_MAPPING_PATH,\"X_train.mymemmap\"))\n",
    "    train_size=int(file_size/(n_mfcc_vectors_per_segment*n_mfcc*8))\n",
    "    \n",
    "    file_size = os.path.getsize(os.path.join(MEMORY_MAPPING_PATH,\"X_validation.mymemmap\"))\n",
    "    validation_size=int(file_size/(n_mfcc_vectors_per_segment*n_mfcc*8))\n",
    "    \n",
    "    file_size = os.path.getsize(os.path.join(MEMORY_MAPPING_PATH,\"X_test.mymemmap\"))\n",
    "    test_size=int(file_size/(n_mfcc_vectors_per_segment*n_mfcc*8))\n",
    "    print(f\"Train, Validation, Test Memory maps of sizes {train_size}, {validation_size}, {test_size} read respectively\")\n",
    "    \n",
    "    X_train = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"X_train.mymemmap\"), dtype='float64', mode='r', shape=(train_size, n_mfcc_vectors_per_segment, n_mfcc))\n",
    "    X_validation = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"X_validation.mymemmap\"), dtype='float64', mode='r', shape=(validation_size,n_mfcc_vectors_per_segment, n_mfcc))\n",
    "    X_test = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"X_test.mymemmap\"), dtype='float64', mode='r', shape=(test_size, n_mfcc_vectors_per_segment, n_mfcc))\n",
    "    y_train = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"y_train.mymemmap\"), dtype='uint8', mode='r', shape=(train_size,))\n",
    "    y_validation = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"y_validation.mymemmap\"), dtype='uint8', mode='r', shape=(validation_size,))\n",
    "    y_test = np.memmap(os.path.join(MEMORY_MAPPING_PATH,\"y_test.mymemmap\"), dtype='uint8', mode='r', shape=(test_size,))\n",
    "\n",
    "\n",
    "    input_example_shape =  X_train[0].shape\n",
    "    input_dtype = np.float64\n",
    "    print(f\"Input example shape : {input_example_shape} Input datatype : {input_dtype}\")\n",
    "\n",
    "    # generator function\n",
    "    def X_train_generator():\n",
    "        return iter(X_train)\n",
    "    def X_validation_generator():\n",
    "        return iter(X_validation)\n",
    "    def X_test_generator():\n",
    "        return iter(X_test)\n",
    "\n",
    "    # create tf dataset from generator fn\n",
    "    X_train_dataset = tf.data.Dataset.from_generator(\n",
    "        generator=X_train_generator,\n",
    "        output_types=input_dtype,\n",
    "        output_shapes=input_example_shape,\n",
    "    )\n",
    "    # create tf dataset from generator fn\n",
    "    X_validation_dataset = tf.data.Dataset.from_generator(\n",
    "        generator=X_validation_generator,\n",
    "        output_types=input_dtype,\n",
    "        output_shapes=input_example_shape,\n",
    "    )\n",
    "    # create tf dataset from generator fn\n",
    "    X_test_dataset = tf.data.Dataset.from_generator(\n",
    "        generator=X_test_generator,\n",
    "        output_types=input_dtype,\n",
    "        output_shapes=input_example_shape,\n",
    "    )\n",
    "\n",
    "    label_example_shape=y_train[0].shape\n",
    "    label_data_dtype=np.uint8\n",
    "    print(f\"Label example shape : {label_example_shape} Label datatype : {label_data_dtype}\")\n",
    "\n",
    "    # generator function\n",
    "    def y_train_generator():\n",
    "        return iter(y_train)\n",
    "    def y_validation_generator():\n",
    "        return iter(y_validation)\n",
    "    def y_test_generator():\n",
    "        return iter(y_test)\n",
    "\n",
    "    # create tf dataset from generator fn\n",
    "    y_train_dataset = tf.data.Dataset.from_generator(\n",
    "        generator=y_train_generator,\n",
    "        output_types=label_data_dtype,\n",
    "        output_shapes=label_example_shape\n",
    "    )\n",
    "    # create tf dataset from generator fn\n",
    "    y_validation_dataset = tf.data.Dataset.from_generator(\n",
    "        generator=y_validation_generator,\n",
    "        output_types=label_data_dtype,\n",
    "        output_shapes=label_example_shape\n",
    "    )\n",
    "    # create tf dataset from generator fn\n",
    "    y_test_dataset = tf.data.Dataset.from_generator(\n",
    "        generator=y_test_generator,\n",
    "        output_types=label_data_dtype,\n",
    "        output_shapes=label_example_shape\n",
    "    )\n",
    "\n",
    "    train_dataset= tf.data.Dataset.zip((X_train_dataset, y_train_dataset))\n",
    "    validation_dataset= tf.data.Dataset.zip((X_validation_dataset, y_validation_dataset))\n",
    "    test_dataset= tf.data.Dataset.zip((X_test_dataset, y_test_dataset))\n",
    "\n",
    "    def RESHAPE(tensor_value,label):\n",
    "        tensor_value=tf.reshape(tensor_value,(input_example_shape[0], input_example_shape[1],1))\n",
    "        tensor_value.set_shape(tensor_value.shape)\n",
    "        return (tensor_value,label)\n",
    "\n",
    "    train_dataset = train_dataset.map(RESHAPE)\n",
    "    validation_dataset = validation_dataset.map(RESHAPE)\n",
    "    test_dataset = test_dataset.map(RESHAPE)\n",
    "\n",
    "    train_batch=train_dataset.batch(32)\n",
    "    validation_batch=validation_dataset.batch(32)\n",
    "    test_batch=test_dataset.batch(32)\n",
    "\n",
    "    print(f\"Dataset created : {time.time()-s} Seconds\")\n",
    "    return train_batch,validation_batch,test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91293d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(MEMORY_MAPPING_PATH=None,ERROR_PATH=None,MODEL_SAVE_DIRECTORY=None,DATASET_PATH=None,CSV_PATH=None,TEST_CSV_PATH=None,remove=True,SAMPLE_RATE=22050,TRACK_DURATION = 30,n_mfcc=13,n_fft=2048,hop_length=512,n_segments=10,n_augmentations=0):\n",
    "    \"\"\"\n",
    "    MEMORY_MAPPING_PATH is where we will store the mfcc numpy arrays of train,test,validation input and labels. [6 files total]\n",
    "    ERROR_PATH -> csv file which will include files that are unreadable or segments inside those files that are corrupt.\n",
    "    MODEL_SAVE_DIRECTORY -> The path where we want to save the model.\n",
    "    If no csv file, assuming mp3/wav files are in respective genre folder where folder name = genre label\n",
    "    If there is a csv file with dataset, columns = [Filename,Genre,Path]\n",
    "    DATASET_PATH must be given if csv file of audio files is not there.\n",
    "    DATASET_PATH AND CSV_PATH need not be given since we will read directly from the csv file - the path of the audio files.\n",
    "    remove -> whether we want to remove existing memory maps or not. This WILL avoid preprocessing. If remove = True and memory maps not in location, will throw error. \n",
    "    SAMPLE_RATE= The number of data points per second in the audio file. Default is 22050.\n",
    "    TRACK_DURATION = 30\n",
    "    n_mfcc=13\n",
    "    n_fft=2048\n",
    "    hop_length=512\n",
    "    n_segments=10\n",
    "    n_augmentations=0\n",
    "    \"\"\"\n",
    "    s=time.time()\n",
    "    #numpy memory mapping gets created, error_csv gets created\n",
    "    if((remove==True) or (CSV_PATH==None) or (MEMORY_MAPPING_PATH==None)):\n",
    "        if(CSV_PATH==None and DATASET_PATH==None):\n",
    "            raise ValueError('Either dataset path or csv path must be given')\n",
    "        elif(CSV_PATH==None):\n",
    "            #csv file will be stored in dataset directory itself\n",
    "            CSV_PATH=os.path.join(DATASET_PATH,\"Audio_Dataset.csv\")\n",
    "            create_csv(DATASET_PATH,CSV_PATH)\n",
    "        else:\n",
    "            pass\n",
    "        if(ERROR_PATH==None):\n",
    "            ERROR_PATH=os.path.join(os.path.split(CSV_PATH)[0],\"Error.csv\")\n",
    "        if(TEST_CSV_PATH==None):\n",
    "            TEST_CSV_PATH=os.path.join(os.path.split(CSV_PATH)[0],\"Test.csv\")\n",
    "        if(MEMORY_MAPPING_PATH==None):\n",
    "            MEMORY_MAPPING_PATH=os.path.split(CSV_PATH)[0]\n",
    "        Preprocess(MEMORY_MAPPING_PATH,ERROR_PATH,CSV_PATH,TEST_CSV_PATH,SAMPLE_RATE,TRACK_DURATION,n_mfcc,n_fft,hop_length,n_segments,n_augmentations)\n",
    "    if(MODEL_SAVE_DIRECTORY==None):\n",
    "        MODEL_SAVE_DIRECTORY=os.path.join(os.path.split(CSV_PATH)[0],\"Model.h5\")\n",
    "        \n",
    "    SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
    "    samples_per_segment = int(SAMPLES_PER_TRACK / n_segments)\n",
    "    n_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "    train_batch,validation_batch,test_batch=create_dataset(MEMORY_MAPPING_PATH,n_mfcc,n_segments,n_mfcc_vectors_per_segment)\n",
    "\n",
    "    df=pd.read_csv(CSV_PATH)\n",
    "    n_classes=len(list(df[\"Genre\"].unique()))\n",
    "    genre_list=sorted(list(df[\"Genre\"].unique()))\n",
    "    del df\n",
    "    input_shape=(n_mfcc_vectors_per_segment, n_mfcc,1)\n",
    "    model=build_model(input_shape,n_classes)\n",
    "    print(f\"Model built with input shape : {input_shape} and output classes : {n_classes}\")\n",
    "\n",
    "    optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimiser,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    print(\"Model compiled\")\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(train_batch,epochs=50,validation_data=validation_batch,verbose=1)\n",
    "    # plot accuracy/error for training and validation\n",
    "    plot_history(history)\n",
    "\n",
    "    # evaluate model on test set\n",
    "    test_loss, test_acc = model.evaluate(test_batch, verbose=2)\n",
    "    #Performance csv.loc[len(index)]=[MODEL_SAVE_DIRECTORY,test_loss,test_acc]  \n",
    "    print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "    with h5py.File(MODEL_SAVE_DIRECTORY, mode='w') as f:\n",
    "        hdf5_format.save_model_to_hdf5(model, f)\n",
    "        f.attrs['Genre_List'] = genre_list\n",
    "        f.attrs[\"SAMPLE_RATE\"] = SAMPLE_RATE\n",
    "        f.attrs[\"TRACK_DURATION\"]=TRACK_DURATION\n",
    "        f.attrs[\"n_mfcc\"]=n_mfcc\n",
    "        f.attrs[\"n_fft\"]=n_fft\n",
    "        f.attrs[\"hop_length\"]=hop_length\n",
    "        f.attrs[\"n_segments\"]=n_segments\n",
    "    print(f\"MODEL SUCESSFULLY SAVED AT {MODEL_SAVE_DIRECTORY}\")\n",
    "    print(f\"Training Execution Time : {time.time()-s} Seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a2c7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH=\"D:/Downloads/MGR Data/Data/GTZAN/Data/genres_original\"\n",
    "SAMPLE_RATE=22050\n",
    "TRACK_DURATION = 30\n",
    "n_mfcc=13\n",
    "n_fft=2048\n",
    "hop_length=512\n",
    "n_segments=10\n",
    "n_augmentations=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e8650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training(DATASET_PATH=DATASET_PATH,remove=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "441e0bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH=\"D:/Downloads/MGR Data/Metadata/Datasets/GTZAN/GTZAN.csv\"\n",
    "MEMORY_MAPPING_PATH = \"D:/Downloads/MGR Data/Metadata/Datasets/Combined/\"\n",
    "ERROR_PATH=\"D:/Downloads/MGR Data/Metadata/Datasets/Combined/Error.csv\"\n",
    "MODEL_SAVE_DIRECTORY=\"D:/Downloads/MGR Data/Models/EXPERIMENT 6.h5\"\n",
    "DATASET_PATH=None\n",
    "TEST_CSV_PATH=None\n",
    "SAMPLE_RATE=22050\n",
    "TRACK_DURATION = 30\n",
    "n_mfcc=13\n",
    "n_fft=2048\n",
    "hop_length=256\n",
    "n_segments=10\n",
    "n_augmentations=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160bfe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training(MEMORY_MAPPING_PATH,ERROR_PATH,MODEL_SAVE_DIRECTORY,DATASET_PATH,CSV_PATH,TEST_CSV_PATH,remove=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MusicRecommendation",
   "language": "python",
   "name": "musicrecommendation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
